from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx

app = FastAPI()

# 游댢 CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://localhost:3000",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


OLLAMA_URL = "http://ollama:11434/api/generate"

class ChatRequest(BaseModel):
    prompt: str
    personality: str  # Nov칳 parameter pre v칳ber osobnosti

@app.get("/")
async def root():
    return {"message": "Backend is running"}

# @app.post("/chat")
@app.post("/api/chat")
async def chat(req: ChatRequest):
    # Mapovanie osobnost칤 na preddefinovan칠 prompt spr치vy
    personalities = {
        "professor": "Odpovedz na t칰to ot치zku ako profesor na univerzite.",
        "friend": "Odpovedz na t칰to ot치zku priate쬽ky a neform치lne.",
        "jokester": "Odpovedz na t칰to ot치zku s humorom a vtipom.",
        "assistant": "Odpovedz na t칰to ot치zku ako asistent s u쬴to캜n칳mi radami."
    }
    # Z칤ska콘 prompt pre vybran칰 osobnos콘
    personality_prompt = personalities.get(req.personality, "Odpovedz na t칰to ot치zku neutr치lne.")
    
    # Kombinuj p칪vodn칳 prompt s osobnostn칳m promptom
    combined_prompt = f"{personality_prompt} {req.prompt}"

    payload = {
        "model": "llama3",  # alebo in칳 model nain코talovan칳 cez Ollama
        "prompt": combined_prompt,
        "stream": False
    }
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.post(OLLAMA_URL, json=payload)
        # return response.json()

        data = response.json()
        return { "reply": data["response"] } 
